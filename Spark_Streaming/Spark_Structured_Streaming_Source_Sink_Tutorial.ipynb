{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03eab517",
   "metadata": {},
   "source": [
    "<img width=\"200\" style=\"float:left\" \n",
    "     src=\"https://upload.wikimedia.org/wikipedia/commons/f/f3/Apache_Spark_logo.svg\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149c61df",
   "metadata": {},
   "source": [
    "# Spark Structured Streaming - Sources and Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42d5fe66",
   "metadata": {},
   "source": [
    "Notebook to explore the different options of querying stream data using <b>Spark Structured Streaming API</b>.\n",
    "\n",
    "As you may remember, we treat this data streams as if they were Spark DataFrames whenever transforming the data and aggregating the data. Now big differences are going to be noticeable whenever reading and writing the information.\n",
    "\n",
    "Our main focus of this notebook is showing examples of this commands and in which situations you need to use them.\n",
    "\n",
    "All the documentation for the API you can find it in the [link](https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html).\n",
    "\n",
    "**WARNING** -- This notebook is not for launching queries/scripts as it is not properly Spark is not properly set up for launching code. It is only meant for showing different options of scripts for Spark Structured Streaming API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6933128",
   "metadata": {},
   "source": [
    "## Sections\n",
    "* [Sources](#0)\n",
    "    * [File Source](#0.1)\n",
    "    * [Socket Source](#0.2)\n",
    "    * [Kafka Source](#0.3)\n",
    "* [Sinks](#1)\n",
    "    * [File Sinks](#1.1)\n",
    "    * [Kafka Sinks](#1.2)\n",
    "    * [Memory Sinks](#1.3)\n",
    "    * [Console Sinks](#1.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228a01cc",
   "metadata": {},
   "source": [
    "<a id='0'></a>\n",
    "## <u>Sources</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca41c72d",
   "metadata": {},
   "source": [
    "In this section we will view the different ways to read files using Spark Structured Streaming.\n",
    "\n",
    "Depending on the type of file, location and data inside of the file you will need to insert different options to make spark posible to read them correctly.\n",
    "\n",
    "The input can be either unstructured data or structured dataframes stored in HDFs.\n",
    "\n",
    "Common options:\n",
    "\n",
    "* <b>maxFilesPerTrigger</b> --> You indicate how many files you are going to read for each iteration.\n",
    "* <b>latestFirst</b> --> When you indicate True then all new files are going to be read first.\n",
    "* <b>maxFilesAge</b> --> Define how old are going to be oldest files to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1836b32d",
   "metadata": {},
   "source": [
    "<a id='0.1'></a>\n",
    "### File Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590091e8",
   "metadata": {},
   "source": [
    "You use this kind of source to read directly from files that are stored either in your file system or in HDFS. \n",
    "\n",
    "It can read different kind of formats:\n",
    "\n",
    "* CSV\n",
    "* JSON\n",
    "* Parquet\n",
    "* ORC\n",
    "* text\n",
    "* textfile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "007a24dd",
   "metadata": {},
   "source": [
    "Examples:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9623c06c",
   "metadata": {},
   "source": [
    "- Normal reading csv from path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50391e2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up the schema with which you are going to read the dataframe\n",
    "csv_schema = StructType().add(\"name\", \"string\").add(\"age\", \"integer\")\n",
    "\n",
    "## Declaring path to the csv\n",
    "path_to_csv = \"/././\"\n",
    "\n",
    "## Read the source from csv located in your file system\n",
    "df_csv = spark \\\n",
    "    .readStream \\\n",
    "    .option(\"sep\", \",\") \\ # you declare how the csv is separating the columns inside\n",
    "    .option(\"header\", \"false\") \\ # you declare if the first line in the csv file is a header\n",
    "    .schema(csv_schema) \\ # you can already declare the spark schema already\n",
    "    .csv(path_to_csv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a977b76e",
   "metadata": {},
   "source": [
    "- Reading part of the text files from hdfs path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd490b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declaring path to the HDFS file where text files are\n",
    "path_to_hdfs = \"hdfs://././\"\n",
    "\n",
    "## Read the source from text files located in HDFS\n",
    "df_csv = spark \\\n",
    "    .readStream \\\n",
    "    .option (\"maxFilesPerTrigger\", 50) \\ # generic spark readstream option\n",
    "    .option (\"latestFirst\", \"true\") \\ # generic spark readstream option\n",
    "    .option (\"maxFilesAge\", \"true\") \\ # generic spark readstream option\n",
    "    .text(path_to_hdfs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69dae617",
   "metadata": {},
   "source": [
    "- Reading JSON from hdfs path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9117e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Declaring path to the HDFS file where JSON files are\n",
    "path_to_hdfs = \"hdfs://././\"\n",
    "\n",
    "## Read the source from JSON located in your file system\n",
    "df_csv = spark \\\n",
    "    .readStream \\\n",
    "    .option (\"allowComments\", \"true\") \\ # if in the json file there are comments inserted\n",
    "    .option (\"multiLine\", \"true\") \\ # if each line is a json document itself\n",
    "    .text(path_to_hdfs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66a0c015",
   "metadata": {},
   "source": [
    "<a id='0.2'></a>\n",
    "### Socket Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b799982",
   "metadata": {},
   "source": [
    "The transmition support control (TCP) is a protocol that enable bidirectional communications between a client and a server. \n",
    "\n",
    "The socket source is a TCP socket client (that is running in the driver) to connect to a TCP server that offers an UTF-8 encoded text-based data stream.\n",
    "\n",
    "You need to specify the host and port to where connect to the TCP server.\n",
    "\n",
    "This kind of connection is not used for production but for testing streaming queries/scripts. Also it doesn't replay uncommitted offset, so in case of failure this type of connection will lose data that is not stored in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5046c6a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple TCP server connection\n",
    "df_socket = spark \\\n",
    "            .readStream\\\n",
    "            .format(\"socket\")\\\n",
    "            .option(\"host\", \"localhost\")\\\n",
    "            .option(\"port\", 9999)\\"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c94d413e",
   "metadata": {},
   "source": [
    "<a id='0.3'></a>\n",
    "### Kafka Source"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b179e48c",
   "metadata": {},
   "source": [
    "With this method we will read from a **Kafka Producer** and a specific topic.\n",
    "\n",
    "You need to define in which ports is set up the Kafka Producer.\n",
    "\n",
    "Also you need to specify the topics from which obtain the different events.\n",
    "\n",
    "You will obtain a rows with the next fields:\n",
    "\n",
    "- key\n",
    "- value\n",
    "- topic\n",
    "- partition\n",
    "- offset\n",
    "- timestamp\n",
    "- timestampType\n",
    "- headers (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa749fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kafka-Spark Integration API \n",
    "\n",
    "df_kafka = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"host1:port1, host2:port2\") \\ # specify ports from where connect to producer\n",
    "        .option(\"subscribe\", \"topic1,topic2\") \\ # specify which topics read from\n",
    "        .option(\"includeHeaders\", \"true\") \\ #include the headers in a column in the resulting dataframe\n",
    "        .option(\"startingOffsets\", \"earliest\") \\ # from where to start reading | default: \"earliest\"-batch , \"latest\"-streaming\n",
    "        .option(\"endingOffsets\", \"latest\") \\ , # to where stop reading\n",
    "        .option(\"maxTriggerDelay\", \"3m\") \\ # maximum time of wait between triggers\n",
    "        .option(\"minPartitions\", \"2\") \\ # minimum number partitions from where to read considering 1:1 relation between spark and topic partitions\n",
    "        .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955425ac",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## <u>Sinks</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53411561",
   "metadata": {},
   "source": [
    "Once we read the information from the sources, the dataframes will suffer from different transformations or aggregations made using the Spark SQL API.\n",
    "\n",
    "Now, after these operations, the resulting dataframes can be either stored in some local file system or HDFS or sent to another streaming and scalable service such as Kafka.\n",
    "\n",
    "You have two kind of sinks:\n",
    "\n",
    "- Reliable sinks for application processes\n",
    "    - File Sink\n",
    "    - Kafka Sink\n",
    "- Unreliable sinks for experimentation\n",
    "    - Memory sink\n",
    "    - Console sink\n",
    "    \n",
    "Common options:\n",
    "\n",
    "* <b>path</b> --> You indicate the directory in your filesystem where to save the information.\n",
    "* <b>chekpointLocation</b> --> You indicate the directory in your filesystem where to checkpointing metadata.\n",
    "* <b>compression</b> --> You indicate the way to compress data, by default is deactivated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4df804b",
   "metadata": {},
   "source": [
    "<a id='1.1'></a>\n",
    "### File Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d394c2",
   "metadata": {},
   "source": [
    "You use this kind of sink to write directly to files that are stored either in your file system or in HDFS. \n",
    "\n",
    "It can read different kind of formats:\n",
    "\n",
    "* CSV\n",
    "* JSON\n",
    "* Parquet\n",
    "* ORC\n",
    "* text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a9afb1",
   "metadata": {},
   "source": [
    "Examples:\n",
    "\n",
    "* Writing a csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9501c9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_of_csv = 'hdfs://./.'\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"csv\") \\ # format of the output\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\ # create checkpoint location just in case there is some failure\n",
    "    .option(\"header\", 'true') \\ # indicates that you want to also save headers\n",
    "    .option(\"sep\", ';') \\ # indicates the way to create separations in the csv file\n",
    "    .option(\"path\", path_of_csv) \\ # path where writing the dataframe\n",
    "    .start() # to initiate the sink you need to launch this command\n",
    "\n",
    "query.awaitTermination() # launch the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9790f123",
   "metadata": {},
   "source": [
    "* Writing a table in parquet in HDFS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3393c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_parquet = 'hdfs://./.'\n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"parquet\") \\ # format of the output\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\ # create checkpoint location just in case there is some failure\n",
    "    .option(\"path\", path_to_parquet) \\ # path where writing the dataframe\n",
    "    .outputMode(\"update\") \\ # specify that you want to update some registry\n",
    "    .start() # to initiate the sink you need to launch this command\n",
    "\n",
    "query.awaitTermination() # launch the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e5d654",
   "metadata": {},
   "source": [
    "<a id='1.2'></a>\n",
    "### Kafka Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c0a9e",
   "metadata": {},
   "source": [
    "With this method we will send the information to a **Kafka Consumer** with a specific topic.\n",
    "\n",
    "You need to define in which ports is set up the Kafka Consumer.\n",
    "\n",
    "Also you need to specify the topics from which obtain the different events.\n",
    "\n",
    "You have the alternative of erasing the option of \"topic\" inside of the query and include a variable inside the value which header is named \"topic\". In that case, it will direct each row depending of the variable topic of the dataframe.\n",
    "\n",
    "You will obtain a rows with the next fields:\n",
    "\n",
    "- key\n",
    "- value\n",
    "\n",
    "Inside of value you will need to include the dataframe that you want to send to Kafka consumer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9840de55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Kafka-Spark Integration API \n",
    "\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .queryName('KafkaQuery') \\\n",
    "    .format(\"kafka\") \\ # specify that is kafka write\n",
    "    .option(\"kafka.bootstrap.servers\", \"host1:port1, host2:port2\") \\ # specify ports from where connect to producer\n",
    "    .option(\"topic\", \"topic1\") \\ # specify which topics read from\n",
    "    .outputMode(\"append\") \\ # specify the way you want to write information\n",
    "    .option(\"checkpointLocation\", \"path/to/checkpoint/dir\") \\ # create checkpoint location just in case there is some failure\n",
    "    .option(\"failOnDataLoss\", \"false\") # use this for testing, never for production\n",
    "    .start()\n",
    "    \n",
    "query.awaitTermination() # launch the query"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b8f59f5",
   "metadata": {},
   "source": [
    "<a id='1.3'></a>\n",
    "### Memory Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8861b36b",
   "metadata": {},
   "source": [
    "This way of writing information creates a temporary table in memory in order to generate the queries that are required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a0f137",
   "metadata": {},
   "outputs": [],
   "source": [
    "query \\\n",
    "    .writeStream \\\n",
    "    .format(\"memory\") \\ # defining the way to save the dataframe in memory\n",
    "    .queryName('memory_table') \\ # this will be the name of the dataframe stored in memory\n",
    "    .outputMode(\"complete\") \\ # specifies to overwrite the dataframe\n",
    "    .option(\"checkpointLocation\", \"path/to/HDFS/dir\") \\\n",
    "    .start()\n",
    "\n",
    "# Now we query from the in memory saved table\n",
    "spark_session.sql('SELECT * FROM memory_table;').count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f55c8b",
   "metadata": {},
   "source": [
    "<a id='1.4'></a>\n",
    "### Console Sinks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89dfe80",
   "metadata": {},
   "source": [
    "Prints results of the query in the console, useful to visualize how the dataframe looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf55f93",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Start running the query that prints the running counts to the console\n",
    "query = df \\\n",
    "    .writeStream \\\n",
    "    .format(\"console\") \\ # defining to visualize the result via console\n",
    "    .outputMode(\"complete\") \\ # specifies to show all the dataframe\n",
    "    .option(\"checkpointLocation\", \"path/to/HDFS/dir\") \\\n",
    "    .option('numRows', 50) \\ # show 50 rows of the dataframe in console\n",
    "    .option('truncate', 'false') \\ # show all the characters inside of the columns\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
